{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f4b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Transactions Sample:\n",
      "                                         Transaction\n",
      "0  [Bananas, Milk, Juice, Oranges, Toothpaste, Co...\n",
      "1  [Apples, Coffee, Juice, Chocolate, Cereal, But...\n",
      "2                [Butter, Bread, Yogurt, Fish, Rice]\n",
      "3      [Bread, Cereal, Apples, Tea, Salt, Chocolate]\n",
      "4       [Chips, Bananas, Rice, Oats, Oranges, Water]\n"
     ]
    }
   ],
   "source": [
    "# [Student: All] Import  all the necessary libraries for data manipulation and mining\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "# [Student: Whitney Gituara 528] \n",
    "# Simulate Transaction Data\n",
    "# Define a pool of 30+ unique supermarket items\n",
    "items_pool = [\n",
    "    'Milk', 'Bread', 'Butter', 'Eggs', 'Cheese', 'Yogurt', 'Apples', 'Bananas',\n",
    "    'Oranges', 'Grapes', 'Chicken', 'Beef', 'Pork', 'Fish', 'Rice', 'Pasta',\n",
    "    'Tomato Sauce', 'Cereal', 'Oats', 'Sugar', 'Salt', 'Coffee', 'Tea', 'Juice',\n",
    "    'Soda', 'Water', 'Chips', 'Cookies', 'Chocolate', 'Soap', 'Shampoo', 'Toothpaste'\n",
    "]\n",
    "\n",
    "# [Student: Whitney Gituara 528]\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize a list to hold all transactions\n",
    "transactions = []\n",
    "\n",
    "# Simulate 3000 transactions\n",
    "for i in range(3000):\n",
    "    # Randomly select a transaction length between 1 and 6 items\n",
    "    transaction_length = random.randint(2, 7)\n",
    "    \n",
    "    #Randomly sample items from the pool without replacement\n",
    "    transaction = random.sample(items_pool, transaction_length)\n",
    "    \n",
    "    # Append the transaction to the list\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Convert the list of lists into a DataFrame for easier visualization (optional step)\n",
    "df_raw = pd.DataFrame({'Transaction': transactions})\n",
    "\n",
    "# Save the raw simulated data to CSV as required\n",
    "df_raw.to_csv('C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/supermarket_transactions.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the first few rows to verify simulation\n",
    "print(\"Raw Transactions Sample:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efdfddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Data Shape: (3000, 32)\n"
     ]
    }
   ],
   "source": [
    "#[Student: Peter Kidiga 341]\n",
    "# 2. Preprocessing: One-Hot Encoding\n",
    "\n",
    "# Initialize the TransactionEncoder from mlxtend\n",
    "te = TransactionEncoder()\n",
    "\n",
    "#  Fit and transform the transaction data into a boolean array\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# Convert the boolean array into a pandas DataFrame (One-Hot Encoded)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Display the shape to confirm 3000 rows and 30+ columns\n",
    "print(f\"\\nEncoded Data Shape: {df_encoded.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d3fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Frequent Itemsets:\n",
      "     support     itemsets  length\n",
      "10  0.150000     (Coffee)       1\n",
      "31  0.150000     (Yogurt)       1\n",
      "20  0.149000       (Pork)       1\n",
      "22  0.147667       (Salt)       1\n",
      "0   0.147000     (Apples)       1\n",
      "11  0.146000    (Cookies)       1\n",
      "7   0.146000    (Chicken)       1\n",
      "6   0.145333     (Cheese)       1\n",
      "30  0.145000      (Water)       1\n",
      "9   0.144667  (Chocolate)       1\n"
     ]
    }
   ],
   "source": [
    "# [Dennis 673]\n",
    "# 3. Generate Frequent Itemsets (Apriori)\n",
    "#  Apply the apriori algorithm with min_support = 0.05, use_colnames=True ensures we get item names instead of column indices\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Add a column for the length of the itemset (useful for filtering later)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "# Sort by support in descending order\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "\n",
    "# Save all frequent itemsets to CSV\n",
    "frequent_itemsets.to_csv(\"C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/frequent_itemsets.csv\", index=False)\n",
    "\n",
    "# Display the top 10 frequent itemsets\n",
    "print(\"\\nTop 10 Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27dd711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Closed Itemsets: 32\n"
     ]
    }
   ],
   "source": [
    "# [Yano ]\n",
    "# 4. Identify Closed Frequent Itemsets\n",
    "#  Logic for Closed Itemsets: An itemset X is closed if NO immediate superset has the SAME support count.\n",
    "# Initialize a list to store closed itemsets\n",
    "closed_itemsets_list = []\n",
    "\n",
    "# Iterate through each frequent itemset (we call this the 'current' itemset)\n",
    "for index, row in frequent_itemsets.iterrows():\n",
    "    current_itemset = row['itemsets']\n",
    "    current_support = row['support']\n",
    "    is_closed = True # Assume it is closed initially\n",
    "    \n",
    "    # Compare against all other frequent itemsets to find supersets\n",
    "    for _, row_check in frequent_itemsets.iterrows():\n",
    "        check_itemset = row_check['itemsets']\n",
    "        check_support = row_check['support']\n",
    "        \n",
    "        # Check if 'check_itemset' is a strict superset of 'current_itemset'\n",
    "        if current_itemset != check_itemset and current_itemset.issubset(check_itemset):\n",
    "            # [Student: Member C] If a superset exists with the SAME support, 'current_itemset' is NOT closed\n",
    "            if current_support == check_support:\n",
    "                is_closed = False\n",
    "                break\n",
    "    \n",
    "    # If the check passed, add to the list\n",
    "    if is_closed:\n",
    "        closed_itemsets_list.append(row)\n",
    "\n",
    "# Convert the list of closed itemsets to a DataFrame\n",
    "closed_itemsets = pd.DataFrame(closed_itemsets_list)\n",
    "\n",
    "# Save closed itemsets to CSV\n",
    "closed_itemsets.to_csv(\"C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/closed_itemsets.csv\", index=False)\n",
    "\n",
    "print(f\"\\nNumber of Closed Itemsets: {len(closed_itemsets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7794973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Maximal Itemsets: 32\n",
      "\n",
      "All files generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# [Gift]\n",
    "# 5. Identify Maximal Frequent Itemsets\n",
    "# Logic for Maximal Itemsets:\n",
    "# An itemset X is maximal if it has NO frequent superset at all.\n",
    "\n",
    "# Initialize a list to store maximal itemsets\n",
    "maximal_itemsets_list = []\n",
    "\n",
    "# Iterate through each frequent itemset\n",
    "for index, row in frequent_itemsets.iterrows():\n",
    "    current_itemset = row['itemsets']\n",
    "    is_maximal = True # Assume maximal initially\n",
    "    \n",
    "    # Check against all other itemsets\n",
    "    for _, row_check in frequent_itemsets.iterrows():\n",
    "        check_itemset = row_check['itemsets']\n",
    "        \n",
    "        # If a strict superset exists (regardless of support value), it is NOT maximal\n",
    "        if current_itemset != check_itemset and current_itemset.issubset(check_itemset):\n",
    "            is_maximal = False\n",
    "            break\n",
    "    \n",
    "    #  If no superset was found, it is maximal\n",
    "    if is_maximal:\n",
    "        maximal_itemsets_list.append(row)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "maximal_itemsets = pd.DataFrame(maximal_itemsets_list)\n",
    "\n",
    "# Save maximal itemsets to CSV\n",
    "maximal_itemsets.to_csv(\"C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/maximal_itemsets.csv\", index=False)\n",
    "\n",
    "print(f\"\\nNumber of Maximal Itemsets: {len(maximal_itemsets)}\")\n",
    "print(\"\\nAll files generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
