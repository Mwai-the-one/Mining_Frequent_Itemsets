{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3659353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Transactions Sample:\n",
      "                                         Transaction\n",
      "0  [Bananas, Milk, Juice, Oranges, Toothpaste, Co...\n",
      "1  [Apples, Coffee, Juice, Chocolate, Cereal, But...\n",
      "2                [Butter, Bread, Yogurt, Fish, Rice]\n",
      "3      [Bread, Cereal, Apples, Tea, Salt, Chocolate]\n",
      "4       [Chips, Bananas, Rice, Oats, Oranges, Water]\n",
      "\n",
      "Encoded Data Shape: (3000, 32)\n",
      "\n",
      "Top 10 Frequent Itemsets:\n",
      "     support     itemsets  length\n",
      "10  0.150000     (Coffee)       1\n",
      "31  0.150000     (Yogurt)       1\n",
      "20  0.149000       (Pork)       1\n",
      "22  0.147667       (Salt)       1\n",
      "0   0.147000     (Apples)       1\n",
      "11  0.146000    (Cookies)       1\n",
      "7   0.146000    (Chicken)       1\n",
      "6   0.145333     (Cheese)       1\n",
      "30  0.145000      (Water)       1\n",
      "9   0.144667  (Chocolate)       1\n",
      "\n",
      "Number of Closed Itemsets: 32\n"
     ]
    }
   ],
   "source": [
    "# [Student: All] Import necessary libraries for data manipulation and mining\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "# [Student: Member A] \n",
    "# 1. Simulate Transaction Data\n",
    "\n",
    "# Define a pool of 30+ unique supermarket items\n",
    "items_pool = [\n",
    "    'Milk', 'Bread', 'Butter', 'Eggs', 'Cheese', 'Yogurt', 'Apples', 'Bananas',\n",
    "    'Oranges', 'Grapes', 'Chicken', 'Beef', 'Pork', 'Fish', 'Rice', 'Pasta',\n",
    "    'Tomato Sauce', 'Cereal', 'Oats', 'Sugar', 'Salt', 'Coffee', 'Tea', 'Juice',\n",
    "    'Soda', 'Water', 'Chips', 'Cookies', 'Chocolate', 'Soap', 'Shampoo', 'Toothpaste'\n",
    "]\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize a list to hold all transactions\n",
    "transactions = []\n",
    "\n",
    "# Simulate 3000 transactions\n",
    "for i in range(3000):\n",
    "    # Randomly select a transaction length between 1 and 6 items\n",
    "    transaction_length = random.randint(2, 7)\n",
    "    \n",
    "    # [Student: Member A] Randomly sample items from the pool without replacement\n",
    "    transaction = random.sample(items_pool, transaction_length)\n",
    "    \n",
    "    # [Student: Member A] Append the transaction to the list\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Convert the list of lists into a DataFrame for easier visualization (optional step)\n",
    "df_raw = pd.DataFrame({'Transaction': transactions})\n",
    "\n",
    "# Save the raw simulated data to CSV as required\n",
    "df_raw.to_csv('C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/supermarket_transactions.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify simulation\n",
    "print(\"Raw Transactions Sample:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "#[Student: Member B]\n",
    "# 2. Preprocessing: One-Hot Encoding\n",
    "\n",
    "# Initialize the TransactionEncoder from mlxtend\n",
    "te = TransactionEncoder()\n",
    "\n",
    "#  Fit and transform the transaction data into a boolean array\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "# [Student: Member B] Convert the boolean array into a pandas DataFrame (One-Hot Encoded)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Display the shape to confirm 3000 rows and 30+ columns\n",
    "print(f\"\\nEncoded Data Shape: {df_encoded.shape}\")\n",
    "\n",
    "# [Student: Member C]\n",
    "# 3. Generate Frequent Itemsets (Apriori)\n",
    "#  Apply the apriori algorithm with min_support = 0.05, use_colnames=True ensures we get item names instead of column indices\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Add a column for the length of the itemset (useful for filtering later)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "# Sort by support in descending order\n",
    "frequent_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "\n",
    "# Save all frequent itemsets to CSV\n",
    "frequent_itemsets.to_csv(\"C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/frequent_itemsets.csv\", index=False)\n",
    "\n",
    "# Display the top 10 frequent itemsets\n",
    "print(\"\\nTop 10 Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head(10))\n",
    "\n",
    "# Student: Member   D]\n",
    "# 4. Identify Closed Frequent Itemsets\n",
    "#  Logic for Closed Itemsets: An itemset X is closed if NO immediate superset has the SAME support count.\n",
    "# Initialize a list to store closed itemsets\n",
    "closed_itemsets_list = []\n",
    "\n",
    "# Iterate through each frequent itemset (we call this the 'current' itemset)\n",
    "for index, row in frequent_itemsets.iterrows():\n",
    "    current_itemset = row['itemsets']\n",
    "    current_support = row['support']\n",
    "    is_closed = True # Assume it is closed initially\n",
    "    \n",
    "    # Compare against all other frequent itemsets to find supersets\n",
    "    for _, row_check in frequent_itemsets.iterrows():\n",
    "        check_itemset = row_check['itemsets']\n",
    "        check_support = row_check['support']\n",
    "        \n",
    "        # Check if 'check_itemset' is a strict superset of 'current_itemset'\n",
    "        if current_itemset != check_itemset and current_itemset.issubset(check_itemset):\n",
    "            # [Student: Member C] If a superset exists with the SAME support, 'current_itemset' is NOT closed\n",
    "            if current_support == check_support:\n",
    "                is_closed = False\n",
    "                break\n",
    "    \n",
    "    # If the check passed, add to the list\n",
    "    if is_closed:\n",
    "        closed_itemsets_list.append(row)\n",
    "\n",
    "# Convert the list of closed itemsets to a DataFrame\n",
    "closed_itemsets = pd.DataFrame(closed_itemsets_list)\n",
    "\n",
    "# Save closed itemsets to CSV\n",
    "closed_itemsets.to_csv(\"C:/Users/HP/Documents/edu/data mining/mining frequent itemsets/Mining_Frequent_Itemsets/closed_itemsets.csv\", index=False)\n",
    "\n",
    "print(f\"\\nNumber of Closed Itemsets: {len(closed_itemsets)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
